{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext,Row\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "file=sc.textFile(\"/Users/fusima/GitHub/sentiment-analysis-dictionary/data/training.1600000.processed.noemoticon.csv\")\\\n",
    "        .map(lambda line: line.split('\"')[1::2])\\\n",
    "        .sample(False,0.2,42)\n",
    "    \n",
    "# Alternativa per estrarre le sottostringhe contenute in una coppia di apici\n",
    "#.map(lambda line: re.findall('\"([^\"]*)\"',line))    \n",
    "\n",
    "tweets = file.map(lambda row: Row(label=int(row[0]), statusid=row[1], text=row[5]))\n",
    "\n",
    "# Create Data Frame\n",
    "tweetsDF = sqlContext.createDataFrame(tweets)\n",
    "\n",
    "# Create Bag of Words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(tweetsDF)\n",
    "\n",
    "# Filter Stopwords\n",
    "stop = stopwords.words('english')\n",
    "def filter_stopwords_and_remove_punctuation(words_array):\n",
    "    out = []\n",
    "    for word in words_array:\n",
    "        if word not in stop:\n",
    "            word_no_punct = re.sub('[^A-Za-z]+', '',word)\n",
    "            if word_no_punct != \"\":\n",
    "                out.append(word_no_punct)\n",
    "    return out\n",
    "\n",
    "# Problema: dopo questa trasformazione passa da SparkRDD a PythonRDD (WTF!?)\n",
    "wordsData2 = wordsData.map(lambda row: \n",
    "                            Row(label=row.label,\n",
    "                                statusid=row.statusid, \n",
    "                                text=row.text,\n",
    "                                words=filter_stopwords_and_remove_punctuation(row.words)))\n",
    "wordsData3= sqlContext.createDataFrame(wordsData2)\n",
    "\n",
    "# Term Frequence\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "featurizedData = hashingTF.transform(wordsData3)\n",
    "\n",
    "# Inverse Term Frequence\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(featurizedData)\n",
    "#rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rawFeatures=featurizedData.map(lambda x: x.rawFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creazione del Modello di Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "#Build the model (cluster the data)\n",
    "clusters = KMeans.train(rawFeatures, k=2, maxIterations=10,\n",
    "                        runs=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "#def error(point):\n",
    "#    center = clusters.centers[clusters.predict(point)]\n",
    "#    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "#WSSSE = rawFeatures.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "#print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "predicted = featurizedData.map(lambda x: Row( label=x.label,\n",
    "                                              statusid=x.statusid,\n",
    "                                              text=x.text,\n",
    "                                              rawFeatures=x.rawFeatures,\n",
    "                                              cluster=clusters.predict(x.rawFeatures)))\n",
    "\n",
    "cluster1 = predicted.filter(lambda x: x.cluster==0).cache()\n",
    "cluster2 = predicted.filter(lambda x: x.cluster==1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = cluster1.map(lambda record: LabeledPoint(record.label, record.rawFeatures))\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Cluster1: \")\n",
    "print(cluster1.count())\n",
    "print(\"Cluster2: \")\n",
    "print(cluster2.count())\n",
    "print(\"Record analizzati: \")\n",
    "print(file.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Tweets from File (nuovi)\n",
    "file=sc.textFile(\"/Users/fusima/GitHub/sentiment-analysis-dictionary/data/tweets-text-sample.csv\")\\\n",
    "        .filter(lambda x: \";\" in x)\\\n",
    "        .map(lambda line: line.split(\";\"))\n",
    "\n",
    "tweets = file.map(lambda line: Row(statusid=line[0],text=line[2]))\\\n",
    "             .filter(lambda x: x.text != \"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
